Create a new service account with the name `pvviewer`. Grant this Service account access to `list` all PersistentVolumes in the cluster by creating an appropriate cluster role called `pvviewer-role` and ClusterRoleBinding called `pvviewer-role-binding`.Next, create a pod called `pvviewer` with the image: `redis` and serviceAccount: `pvviewer` in the default namespace.

- ServiceAccount: pvviewer
- ClusterRole: pvviewer-role
- ClusterRoleBinding: pvviewer-role-binding
- Pod: pvviewer
- Pod configured to use ServiceAccount pvviewer ?

- serviceaccount 생성

```yaml
controlplane ~ ➜  k create serviceaccount pvviewer
serviceaccount/pvviewer created
```

- clusterrole 생성

```yaml
controlplane ~ ➜  kubectl create clusterrole pvviewer-role --verb=list --resource=PersistentVolumes
clusterrole.rbac.authorization.k8s.io/pvviewer-role created
```

- clusterrolebinding 생성

```yaml
controlplane ~ ➜  kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --user=pvviewer
clusterrolebinding.rbac.authorization.k8s.io/pvviewer-role-binding created
```

- Pod 생성

```yaml
controlplane ~ ➜  k run pvviewer --image redis --dry-run=client -o yaml > pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvviewer
  name: pvviewer
spec:
  serviceAccountName: pvviewer
  automountServiceAccountToken: false
  containers:
  - image: redis
    name: pvviewer
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

controlplane ~ ➜  k apply -f pod2.yaml 
pod/pvviewer created

controlplane ~ ➜  k get po
NAME       READY   STATUS    RESTARTS   AGE
pvviewer   1/1     Running   0          119s
```

List the `InternalIP` of all nodes of the cluster. Save the result to a file `/root/CKA/node_ips`.

Answer should be in the format: `InternalIP of controlplane`<space>`InternalIP of node01` (in a single line)

```yaml
controlplane ~ ➜  k get no -o wide
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready    control-plane   31m   v1.26.0   192.20.147.9    <none>        Ubuntu 20.04.5 LTS   5.4.0-1101-gcp   containerd://1.6.6
node01         Ready    <none>          31m   v1.26.0   192.20.147.12   <none>        Ubuntu 20.04.5 LTS   5.4.0-1101-gcp   containerd://1.6.6
```

```yaml
ccontrolplane ~ ➜  k get no -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
192.20.147.9 192.20.147.12

controlplane ~ ➜  k get no -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips

controlplane ~ ➜  cat /root/CKA/node_ips 
192.20.147.9 192.20.147.12
```

Create a pod called `multi-pod` with two containers.Container 1, name: `alpha`, image: `nginx`Container 2: name: `beta`, image: `busybox`, command: `sleep 4800`Environment Variables:container 1:`name: alpha`Container 2:`name: beta`

- Pod Name: multi-pod
- Container 1: alpha
- Container 2: beta
- Container beta commands set correctly?
- Container 1 Environment Value Set
- Container 2 Environment Value Set

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-pod
  name: multi-pod
spec:
  containers:
  - image: nginx
    name: alpha
    env:
    - name: name
      value: "alpha"
    resources: {}
  - image: busybox
    name: beta
    command: ["sleep", "4800"]
    env:
    - name: name
      value: "beta"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

```yaml
controlplane ~ ➜  k get po
NAME        READY   STATUS    RESTARTS   AGE
multi-pod   2/2     Running   0          10s
pvviewer    1/1     Running   0          11m
```

Create a Pod called `non-root-pod` , image: `redis:alpine`runAsUser: 1000fsGroup: 2000

- Pod non-root-pod fsGroup configured
- Pod non-root-pod runAsUser configured

```yaml
controlplane ~ ➜  k run non-root-pod --image redis:alpine --dry-run=client -o yaml > pod4.yaml

controlplane ~ ➜  vi pod4.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  **securityContext:
    runAsUser: 1000
    fsGroup: 2000**
  containers:
  - image: redis:alpine
    name: non-root-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

controlplane ~ ➜  k apply -f pod4.yaml 
pod/non-root-pod created

controlplane ~ ➜  k get po
NAME           READY   STATUS    RESTARTS   AGE
multi-pod      2/2     Running   0          2m44s
non-root-pod   1/1     Running   0          16s
pvviewer       1/1     Running   0          13m
```

We have deployed a new pod called `np-test-1` and a service called `np-test-service`. Incoming connections to this service are not working. Troubleshoot and fix it.Create NetworkPolicy, by the name `ingress-to-nptest` that allows incoming connections to the service over port `80`.

Important: Don't delete any current objects deployed.

- Important: Don't Alter Existing Objects!
- NetworkPolicy: Applied to All sources (Incoming traffic from all pods)?
- NetWorkPolicy: Correct Port?
- NetWorkPolicy: Applied to correct Pod?

```yaml
Labels:           run=np-test-1
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
    - Ingress
	ingress:
    - ports:
      - protocol: TCP
        port: 80
```

Taint the worker node `node01` to be Unschedulable. Once done, create a pod called `dev-redis`, image `redis:alpine`, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called `prod-redis` and image: `redis:alpine` with toleration to be scheduled on `node01`.

key: `env_type`, value: `production`, operator: `Equal` and effect: `NoSchedule`

- Key = env_type
- Value = production
- Effect = NoSchedule
- pod 'dev-redis' (no tolerations) is not scheduled on node01?
- Create a pod 'prod-redis' to run on node01

```yaml
controlplane ~ ➜  kubectl taint nodes node01 env_type=production:NoSchedule
node/node01 tainted
```

```yaml
controlplane ~ ➜  k run dev-redis --image redis:alpine
pod/dev-redis created

controlplane ~ ➜  k get po -o wide 
NAME           READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
dev-redis      1/1     Running   0          15s   10.244.0.4     controlplane   <none>           <none>
multi-pod      2/2     Running   0          15m   10.244.192.2   node01         <none>           <none>
non-root-pod   1/1     Running   0          13m   10.244.192.3   node01         <none>           <none>
np-test-1      1/1     Running   0          12m   10.244.192.4   node01         <none>           <none>
pvviewer       1/1     Running   0          26m   10.244.192.1   node01         <none>           <none>
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: prod-redis
  name: prod-redis
spec:
  containers:
  - image: redis:alpine
    name: prod-redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
status: {}
```

```yaml
controlplane ~ ➜  k get po -o wide 
NAME           READY   STATUS    RESTARTS   AGE    IP             NODE           NOMINATED NODE   READINESS GATES
dev-redis      1/1     Running   0          3m8s   10.244.0.4     controlplane   <none>           <none>
multi-pod      2/2     Running   0          18m    10.244.192.2   node01         <none>           <none>
non-root-pod   1/1     Running   0          16m    10.244.192.3   node01         <none>           <none>
np-test-1      1/1     Running   0          15m    10.244.192.4   node01         <none>           <none>
prod-redis     1/1     Running   0          6s     10.244.192.5   node01         <none>           <none>
pvviewer       1/1     Running   0          29m    10.244.192.1   node01         <none>           <none>
```

Create a pod called `hr-pod` in `hr` namespace belonging to the `production` environment and `frontend` tier .image: `redis:alpine`

Use appropriate labels and create all the required objects if it does not exist in the system already.

- hr-pod labeled with environment production?
- hr-pod labeled with tier frontend?

```yaml
controlplane ~ ➜  k create ns hr
namespace/hr created

controlplane ~ ➜  k run hr-pod -n hr --image redis:alpine --labels=environment=production,tier=frontend
pod/hr-pod created
```

```yaml
controlplane ~ ➜  k get po -n hr  --show-labels 
NAME     READY   STATUS    RESTARTS   AGE     LABELS
hr-pod   1/1     Running   0          4m39s   environment=production,tier=frontend
```

A kubeconfig file called `super.kubeconfig` has been created under `/root/CKA`. There is something wrong with the configuration. Troubleshoot and fix it.

- Fix /root/CKA/super.kubeconfig

```yaml
controlplane ~ ➜  vi /root/CKA/super.kubeconfig

server: https://controlplane:6443
```

We have created a new deployment called `nginx-deploy`. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.

- deployment has 3 replicas

```yaml
controlplane ~ ➜  kubectl scale --replicas=3 deployment nginx-deploy 
deployment.apps/nginx-deploy scaled
```

```yaml
controlplane ~ ➜  vi /etc/kubernetes/manifests/kube-controller-manager.yaml
-> 1 -> l 찾아서 수정..

controlplane ~ ➜  k get deployments.apps 
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy   3/3     3            3           11m
```
